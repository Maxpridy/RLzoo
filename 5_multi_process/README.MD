# multi processing

여기에선 RL에서 멀티프로세싱을 구현한 구현체들에 대해서 알아보자

multi processing을 이용하는 알고리즘들은 기본적으로 일반화 성능 등에서 뛰어날 수 밖에 없지만 나같은 컴퓨팅 자원도 부족하고 그만큼 무거운 환경을 돌릴수도 없는 사람은 지금까지 알아보기도 좀 어려웠다. 마치 GPT-3를 학습시키거나 하는 이야기에 가깝다고 느껴진다. 하지만 여러 구현체가 나오면서 간접적으로 어떻게 구현하는것이 좋아보인다~ 정도는 알 수 있게 되었다고 생각되어서 단락을 작성하게 되었다.

사실 멀티쓰레딩이 멀티프로세싱보다 빠르다. 더 메모리 효율이 좋다 이런 말들이 있지만 강화학습에서 env를 여러개로 띄우는 상황에서 멀티쓰레딩이 얼마나 의미가 있을까 싶긴하다. 결국 대부분의 경우에서 멀티프로세싱으로 env를 여러개 띄우는식으로 진행이된다.

1, 2번에선 임팔라, vtrace에 대한 이야기
3, 4번에선 PPO, APPO에 대한 이야기를 할것이다.
5번은 사실 그냥 넣어봤다. 많은 구현이 있는곳이라 참고할만하기 때문에...

1. torchbeast(IMPALA)
2. HandyRL(IMPAlA)
3. paris팀(Synchronous-PPO)
4. obs-tower2(Synchronous-PPO)
5. RLlib(IMPALA/APPO/PPO)