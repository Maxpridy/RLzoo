# torchbeast

torchbeast는 페이스북에서 내놓은 IMPALA 구현체이다. 벌써 3년이나 지난 논문이지만 아직도 많이 사용되는듯 하다.

## 특징

IMPALA는 보통 비동기 actor들과 (여러) learner로 구성된다. 그 통신을 무엇으로 하느냐는 구현마다 조금씩 다른것같다.

토치비스트에서는 monobeast와 polybeast가 있는데, torch의 multiprocessing을 이용한 구현은 monobeast, gRPC를 이용한 구현은 polybeast인듯 하다.

사실 monobeast처럼 구현하는건 여러곳에서 쉽게 볼 수 있는데, 대부분의 구현이 이와 유사하거나 비슷하다. torch나 python의 multiprocessing을 사용한다.

## GIL

여기서 언급하지 않을 수 없는 부분이 있는데 기본적으로 파이썬은 멀티쓰레딩을 지원하지 않는다. GIL이라고 부르는 시스템을 사용하기 때문이다. 내가 이 부분에 대해 알아보면서 가장 궁금했던건, '그래 무슨말인지는 알겠는데 왜 자바나 다른 언어는 안그래?'였는데 자바와 파이썬은 GC 구현이 다르다고 한다. 파이썬은 여러 쓰레드가 참조하기 시작하면 혼란을 걷잡을 수 없으며 귀도씨가 말했듯이 싱글스레드에서 성능을 저하시키지 않는 방법은 아직도 GIL을 없애지 못한걸보면 아직까지는 없는듯하다. 아무튼 이 부분은 cpython에 조예가 깊지 않은이상 깊게 말하긴 애매한것같다. 나도 어렴풋이만 알고있을뿐 결국 중요한것들은 저걸 구현하는분들이 잘 알고계실듯 하다.

https://velog.io/@litien/GIL-Java%EC%97%90%EB%8A%94-%EC%97%86%EB%8D%98%EB%8D%B0  
이 글이 내 궁금함을 조금은 풀어준것 같다. 

## v-trace

한번도 정리한적이 없는것같아서 vtrace에 대해서도 약간 정리하고 가면 좋을것 같다. 기본적으로 vtrace라는게 왜 존재하냐면 비동기 actor-learner 상황에서 정책의 차이가 발생하기 때문이다. actor는 단순히 행동을 하는 역할이기 때문에 보통 learner가 좀 더 학습의 측면에서 앞서게 된다. 이 차이를 극복하기 위해 vtrace나 이런 비슷한 개념을 사용하게 된다. 

논문의 v-trace 단락을 조금 읽어보자. 

첫번째 문장은 Off-policy learning ~ 인데 off policy인 이유는 learner와 actor간의 policy-lag이 생기기 때문에 두 policy가 다르다 라고 이야기하는듯 하다. 사실 ppo도 1epoch 학습 후에는 점점 데이터와 policy가 벌어지기 시작하는데 여러 제약을 둠으로써 괜찮다 라는 이야기를 하는듯하다.

그 다음 To this end, we introduce a novel offpolicy actor-critic algorithm for the learner, called V-trace. 란다. 러너를 위한 알고리즘이다.

MDP는 공통적인 부분이니 특별한건 없어보인다. 

이후 설명은 off-policy의 목적을 이야기한다. 임팔라의 풀네임은 Importance Weighted Actor-Learner Architecture이다. 대부분 이런 용어가 나오면 L와 B가 있으면 "B로 샘플링해서 L을 학습시키고 싶다."라는 이야기이다. 이 경우엔 B는 behaviour policy이고 L은 learner라고 보면 될 것 같다. 

4번부터가 진짜 수식인데, 여기서 로와 c가 나오는데 이 값들이 임팔라의 핵심이라고 할 수 있다. 뮤로 샘플링된 값이기 때문에 파이/뮤(Importance weight)를 곱해주는 과정을 거친다. 그 값들을 로와 c로 부른다. n-step td를 가정하는 식이라서 복잡해보이지만 1-step으로 바꿔보면 사실 로만 남고 굉장히 간단해진다. 

그 아래는 on-policy일 때에는(뮤==파이) 일반적인 벨만 식으로 쓸 수 있다고 나와있다. 온라인과 오프라인에 같은 알고리즘을 적용할 수 있는게 장점이란다.

그 아래엔 로와 c는 다른 역할을 한다고 한다. 식을 보면 알 수 있듯이 로는 td 전체에 영향을 주고 c는 n-step td식 속에서 꽤 많은 감쇄가 되는듯하다. 그 역할은 본문과 AppendixA에 걸쳐 길게 설명되어있지만 읽어는 보지만 이해는 음....  
일단 그래도 읽어보면, 로와 c의 상한에 대해서 설명한다. 마치 ppo에서 clip을 어디까지 하느냐와 비슷한 느낌인것같다. torchbeast 구현에선 c의 상한은 1.0으로 쓰는것같고 rho의 상한은 기본값은 1인데 바꿀수 있게 되어있다. 테스트에서 3.7을 쓰는거보니 1보다 크면 어떤 효과가 있긴있는듯. 확실히 c의 상한은 1보다 크거나 하면 문제가 발생할것 같기는 하다. 실험에서 로를 키워봤는데 별로 안좋았다는듯

아무튼 그 아래는 식에 대한 이야기들이 있고... 실험에대한 이야기가 있다. 뭐 필요할때 살펴보면 될것같다.

## monobeast

모노비스트는 하나의 model을 actor가 공유하는 시스템으로 구현했고, learner는 파이썬 threading으로 구현했다. 왜 느린 버전인지 두쪽 다 납득이 가는 부분이다. 


## polybeast







## multi processing

아무튼 GIL은 언어 자체의 문제니 좀 더 무거운 멀티프로세스에 대한 구현으로 넘어간다. 이 이야기는 여기서 짧게 할게 아니라 다양한 구현이 존재하는만큼 다양하게 이야기 할 수 있을것 같다.