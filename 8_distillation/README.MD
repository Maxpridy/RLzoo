# 8. distillation

이걸 뭐라고 불러야할지 잘 모르겠다. lux 1위가 이야기하길 teacher student? 내가 이미지쪽 경량화에 약해서 그쪽에선 이런 이름으로 불릴텐데 그것과 약간 유사할지도 모르겠다. 뭐 방식에 따라 커리큘럼 러닝이니 뭐니 하는 용어들도 있고... 일단 distillation이란 단어가 많이 사용되는걸로 알고있기 때문에 통칭해서 부르겠다.

이것을 알아보려는 배경을 말해보면 가장 큰 이유는 lux 대회의 우승코드 때문이다. lux 우승자는 아주 적은 컴퓨팅(강화학습 분야 기준에서, 3080 두개쯤?)으로 우승을 차지했다. 그것도 2위와 꽤나 격차있게... 생각보다 action space를 줄인다던지 하는 문제를 쉽게 만드는 타협도 거의 하지 않았다. 그럼 대체 어떻게 했느냐? 의 답을 1위가 이야기하지 않았나 싶다. 

코드는 모노비스트 기반이고, 거기서 정말 훌륭한 여러가지 방법이 섞여있다. phase를 나눠서 점차 크게, reward함수도 다르게 학습시켜 나가는 방법을 사용했는데 살펴보니 생각보다 phase를 많이 나눈것도 아니다. 초창기에 좋은 액션을 찾아가는게 중요해보이지만 사실 2위도 많은 학습 자원을 들였을텐데 왜 1위가 잘 되었는가? 를 이야기하기엔 결국 이 구조말고는 설명하기가 어려워보였다. 1위 본인도 댓글에서 이 구조가 1위라는 순위에 가장 큰 영향을 주지 않았을까 예측했다.

이 개념은 사실 옛날부터 되게 중요하다고 생각했는데, 근본적으로 목표라는것에 대한 설계에서 우리는 당연하다고 생각하는 중간단계에 대해 에이전트는 당연하게도 지식이 없기때문에 그러한것을 경유해야한다는 조건을 만족하려면 부수적인 reward와 엄청난 exploration이 필요해진다. 그렇다고 계속 번잡한 리워드를 주면 줄수록 궁극적인 목표에는 도달하기 어려워진다. 이는 episode step이 상당히 긴 env를 학습시킬수록 더 크게 느껴지는것 같다.


여기서 살펴보려고 하는 주요 개념을 짚어보면
- 작은 모델에서 큰 모델로
- 복잡한 리워드에서 간단한(궁극적인) 리워드로(부분 목표를 위해)
- teacher모델을 student모델로

첫번째줄의 중요성은 학습 속도이다. 이것을 주제1이라고 부르겠다. 이게 우습게 볼 주제가 아니라는것은 강화학습 학습시켜본 사람들은 누구나 알것이다. 내가 알고싶은건 이 가속의 속도가 궁금한것이다. 이 부분에 대한 실험을 직접 진행할 예정이다.

두번째줄은 주제2라고 부르겠다. 이것은 어찌보면 알파고 같은 굉장히 간단한 reward 시스템으로 학습시켰던 논문들에 적용할 수 있는 방법이면서 만약 게임이 좀 더 복잡하다면 행동을 유도할 수 있는 좋은 방법이다. exploration으로 찾기 힘든 행동들을 강제로 학습시킬 수 있다는 점에서 이것또한 학습 속도 개선에 큰 영향을 미칠 수 있다.

세번째는 근본적인 이 모델의 방법론으로, 뭐 별거 없다. student를 KL divergence로 학습시키는것이다.

직접 실험을 진행해볼 예정인데 사실 env를 어떤걸 선택할지, 직접 만든다면 어떤 env를 만들어야 실험이 잘될지 등이 되게 고민이 많이된다. 하지만 굉장히 중요한 부분이기 때문에 env를 만드는 과정에 대한 고민도 담을 예정이다.

## 일단은 존재하는 env를 활용해보자

일단 있는 env로 해보면서 만드는건 나중에 하자.

주제1같은 경우 간단해도 상관 없을것같다. 일단 진행해볼 생각이 드는건 카트폴v1 생각이 드는데 일단 이거저거 해보는게 목적이기 때문에 해보자는 생각이 든다. 목표는 'robust하게 500에 다가가는 속도가 얼마나 안정적이고 빠른가?' 정도가 간단한 예로 생각된다.

주제2 같은 경우에는 마운틴카같은 이상한 액션이 필요한 경우가 어울린다고 생각된다. 추진력이 필요한 이 환경은 조금 색다른 액션 시퀀스가 필요하다. 이것도 실험을 진행해보자.

실험을 진행할 때 고려해야할 사항은 어떤 구조로 학습을 할것인가인데 구체적으로는 phase를 얼마나 나눌것인지, 한 phase마다 얼마동안 학습할것인지, kl loss는 어떻게 다루는것이 가장 좋을지 등에 대해서 고민을 해봐야할것같다. 

실험은 노승은님의 ppo 구현체를 내가 실험용으로 자주 쓰기 때문에 사용할 예정이다. 감사합니다..

- 1~2번 코드. 간단한 실험 후

1. phase 길이를 너무 짧게 할순 없음(학습이 안됨). 결국 학습은 환경과 인터렉션하면서 하는거지 kl loss로부터 간단하게 배울 수 있는게 아님
2. 모델 크기에 따라 점진적으로 더 빠르게 학습한다? 이건 문제가 너무 쉬워서 확인하기가 어려움. 하지만 해보면서 그럴것 같다는 생각이 들었음
3. teacher가 존재할때 안정적이라는 생각이 들었음. 반대로 최고점(500)을 잘 달성하지 못하는 특징도 보였음. 사진 파일 capture1는 1번 코드 학습시킬때의 모습인데 강화학습이 그렇듯이 이전까지는 500을 계속 잘 달성하다가 삐끗하는 모습을 보이기도 한다. 하지만 2번은 phase가 넘어갈때를 제외하면 그런 모습이 없다. 사실 1000 epi가 굉장히 짧기때문에 이 사이에 큰 변화가 없었을수도 있다.
4. 그냥 생각만 해본것인데, state가 굉장히 다양한 env라면 마주치기 힘든 state에 대한 학습이 새로운 student에게는 어려운 일이 아닐까 싶은데 이 부분이 단점이 될까 모르겠다. phase를 짧게 하는것이 매력적이여 보일 수 있지만 이런 부분들에서 단점이 부각될거라 생각된다.

## lux도 조금 해보자

일단 코드도 더 살펴볼 겸 1위 코드를 응용해서 조금 실험을 진행해봐도 될것같다.

예를들어 나는 카트를 꼭 쓰면서 이기고싶다. 라고 한다면 1phase때 카트를 사용하는것을 학습시키면 2phase 3phase에서도 계속 사용을 할것이다. 궁금해지는점은 정말 카트가 비효율적이라면 알아서 4, 5phase가 되면 사라질까 하는 점이다. 실제로 1위의 결과에선 카트 생산을 하지 않는다. 

이처럼 당연한 이야기지만 주의해야할점도 있다. 알파고에서 보여줬듯이 인간이 옳다고 생각하는 방법이 사실은 완벽하진 않다는것이다. 그런 방법을 학습시키는것은 궁극적으로 성능의 손해로 이어질 수 있다. 이건 참 어려운 일이다. lux에서는 카트를 생산하지 않는것이 명백히 좋은걸로 보이지만 이 생각또한 틀릴수도 있다.

근데 이거 컴퓨팅자원이 적다해도 나는 힘들지도... 일단은 줄여서 실행이라도 되니까 다행이긴한데



## 만든다면 어떤 env가 적절할까?

작성예정