# 8. distillation

이걸 뭐라고 불러야할지 잘 모르겠다. teacher student? 내가 이미지쪽 경량화에 약해서 그쪽에선 이런 이름으로 불릴텐데 그것과 약간 유사할지도 모르겠다. 일단 distillation이란 단어가 많이 사용되는걸로 알고있기 때문에 통칭해서 부르겠다.

이름이야 어찌되었든 여기서 살펴보려고 하는 주요 개념을 짚어보면
- 작은 모델에서 큰 모델로
- 복잡한 리워드에서 간단한(궁극적인) 리워드로
- teacher모델을 student모델로

첫번째줄의 중요성은 학습 속도이다. 이것을 주제1이라고 부르겠다. 이게 우습게 볼 주제가 아니라는것은 강화학습 학습시켜본 사람들은 누구나 알것이다. 내가 알고싶은건 이 가속의 속도가 궁금한것이다. 이 부분에 대한 실험을 직접 진행할 예정이다.

두번째줄은 주제2라고 부르겠다. 이것은 어찌보면 알파고 같은 굉장히 간단한 reward 시스템으로 학습시켰던 논문들에 적용할 수 있는 방법이면서 만약 게임이 좀 더 복잡했다면(바둑이 단순한 게임인건 사실이기 때문에) 행동을 유도할 수 있는 좋은 방법이다. exploration으로 찾기 힘든 행동들을 강제로 학습시킬 수 있다는 점에서 이것또한 학습 속도 개선에 큰 영향을 미칠 수 있다.

세번째는 근본적인 이 모델의 방법론으로, 뭐 별거 없다. 모델끼리 KL divergence로 학습시키는것이다.

직접 실험을 진행해볼 예정인데 사실 env를 어떤걸 선택할지, 직접 만든다면 어떤 env를 만들어야 실험이 잘될지 등이 되게 고민이 많이된다. 하지만 굉장히 중요한 부분이기 때문에 env를 만드는 과정에 대한 고민도 담을 예정이다.

## 일단은 존재하는 env를 활용해보자

일단 있는 env로 해보면서 만드는건 나중에 하자.

주제1같은 경우 간단해도 상관 없을것같다. 일단 진행해볼 생각이 드는건 카트폴v1 생각이 드는데 일단 이거저거 해보는게 목적이기 때문에 해보자는 생각이 든다. 목표는 'robust하게 500에 다가가는 속도가 얼마나 안정적이고 빠른가?' 정도가 간단한 예로 생각된다.

주제2 같은 경우에는 마운틴카같은 이상한 액션이 필요한 경우가 어울린다고 생각된다. 추진력이 필요한 이 환경은 조금 색다른 액션 시퀀스가 필요하다. 이것도 실험을 진행해보자.

실험을 진행할 때 고려해야할 사항은 어떤 구조로 학습을 할것인가인데 구체적으로는 phase를 얼마나 나눌것인지, 한 phase마다 얼마동안 학습할것인지, kl loss는 어떻게 다루는것이 가장 좋을지 등에 대해서 고민을 해봐야할것같다. 


## 만든다면 어떤 env가 적절할까?

작성예정
