# 9. value estimation

이건 강화학습의 근본에 가까운 이야기들이라고 할 수 있다. 학습과 정말 가까운 부분이기도 하고..

원래는 이 주제를 더 일찍 작성하려고 했는데 lux 코드보면서 감탄하여 더 이상은 미룰수가 없게되었다. 

1. td-lambda
2. upgo
3. vtrace

코드는 노승은님의 minimalRL을 기반으로 구현했다. vtrace는 그냥 가져왔다.

일단 생각나는것들부터 작성하겠다. 사실 다 비슷하고 별차이 없는데 upgo에 대해 생각해보려고 만든게 크다. 

td_lambda는 TD와 MC의 장점을 섞는 아주 대단한 방법이다. 이에 대한 설명은 실버 강의를 들으면 대략적으로 이해할 수 있다. 수식에 1과 0을 넣어보면 직관적으로 TD와 MC의 식이 나오는데, 이 사이에서 둘의 장점을 취하는것이다. 최근 lux에서 훌륭한 성능을 낸 코드에서 0.9로 설정해놨길래 똑같이 해봤다. 이 경우엔 거의 MC라고 볼 수 있다. 

upgo는 약간 위에서 0.9로 설정했다면 여기선 반대의 철학이라고 볼 수 있는데 value function이 높게 평가한다면 bootstraping(원래는 랜덤 샘플링의 의미인데 이 분야에서 그냥 더 안보고 eval 해버리기를 뜻하기도 함)해버리는것이다. 

lux 1위는 위에 말했듯이 td_lambda에서 lmb를 0.9로 한것과 upgo를 동시에 loss에 포함했다. 이게 무슨 의미일까? 수식을 찬찬히 살펴보면 양쪽의 철학이 느껴진다. 전자는 실제 reward를 더 참고할것이고, 후자는 vf를 더 참고할것이라는것이다. 이는 양쪽의 장점을 다 가져가겠다는 이야기가 아닐까 싶다. 

실제로 학습을 시켜보면 2번 upgo는 단일로는 약간 불안한 느낌이 든다. 역시 문제가 쉽다보니 뭐가 더 낫다는 실험까지는 어렵지만 일단 3번 코드처럼 섞어도 잘 학습이 되는건 확실하다.